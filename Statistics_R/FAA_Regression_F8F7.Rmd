---
title: "Assumptions of Multiple Linear (OLS) Regression for FAA (F8-F7)"
output:
  html_notebook: default
  html_document:
    df_print: paged
  pdf_document: default
---
```{r}
# LOAD REQUIRED PACKAGES
library(lmtest)
library(readxl)
library(kableExtra)
library(car)
library(nortest)
library(MASS)
library(broom)

# Import data
exportdirxls <- "D:\\MPI_LEMON\\EEG_MPILMBB_LEMON\\EEG_Statistics\\DataLemon.xlsx" # Path to dataLemon file
Data <- data.frame(read_excel(exportdirxls, 1, col_names = TRUE))
row.names(Data) <- Data[,1] # set row names
Data <- Data[,-1] # delete subject variable
Data[,19] <- factor(Data[,19]) # make categorical variables into factors
Data[,20] <- factor(Data[,20])
Data[,21] <- factor(Data[,21])

# Create separate datasets for each model, without missing values
na.ham <- which(is.na(Data$Hamilton.Scale)) # find two missing values in Hamilton.Scale
na.skid <- which(is.na(Data$SKID.Diagnoses)) # find one missing value in SKID.Diagnoses
na <- c(na.ham, na.skid) # combine three missing values
Data.no.na <- Data[-na, ] # remove those 3 subjects as they cannot be entered into regression
dataF8F7 <- Data.no.na[,-c(1:3)]
N = nrow(dataF8F7)

# RUN FOUR OLS REGRESSION MODELS, ONE FOR EACH ELECTRODE PAIR. ONLY F8-F7 HERE
summary(full.modelF8F7 <- lm(FAA.F8F7 ~ ., data = dataF8F7))
anova(full.modelF8F7) # anova table
```


## **Checking of Assumptions for Ordinary Least Squares (OLS) Regression**

The **diagnostic plots** show residuals in four different ways ([1](http://www.sthda.com/english/articles/39-regression-model-diagnostics/161-linear-regression-assumptions-and-diagnostics-in-r-essentials/)):

1. Residuals vs Fitted. The predicted Y-values ($\hat{y}$) are on the X-axis, and the residuals (*e*) are on the Y-axis. Used to check the linear relationship assumptions. A horizontal line, without distinct patterns is an indication for a linear relationship.

2. Normal Q-Q. The ordered observed standardised residuals are on the Y-axis, and the ordered theoretical residuals (expected residuals if they are truly normally distributed) are on the X-axis. Used to examine whether the residuals are normally distributed. It’s good if residuals points follow the straight dashed line.

3. Scale-Location (or Spread-Location). Used to check the homogeneity of variance of the residuals (homoscedasticity). Horizontal line with equally spread points is a good indication of homoscedasticity.

4. Residuals vs Leverage. Used to identify influential cases, that is extreme values that might influence the regression results when included or excluded from the analysis. This plot will be described further in the next sections.

The four plots show the top 3 most extreme data points labeled with with the row numbers of the data in the data set. They might be potentially problematic.

```{r}
par(mfrow = c(2,2))
plot(full.modelF8F7, las = 1)
```

### 1. Unusual Observations
Unusual observations may disproportionately influence the outcomes of the models.
Multiple linear regression is not very robust against these types of observations.

In  the  absence  of  outliers  and  with  the  fulfillment  of  the  assumptions  of  zero mean, constant variance, and uncorrelated errors the OLS provides the Best Linear Unbiased Estimators (BLUE) of the regression parameters. But any anomalous point can disproportionately pull the line and distort the predictions. Detection of outlying observations is a very  essential  part  of  good  regression  analysis.

An influential observation is one which either individually or together with several other observations has a demonstrably larger impact on the calculated values of various estimates (e.g., coefficient, standard errors, t-values) than to the case for most of the other observation (Belsley et al., 1980).

#### 1.1 Outliers
An outlier is an observation with a large absolute residual. Outliers can be identified by examining the standardized residual, which is the residual divided by its estimated standard error. Standardized residuals can be interpreted as the number of standard errors away from the regression line. Observations whose standardized residuals are greater than 3 in absolute value are possible outliers (James et al. 2014). It would be expected that only 0.2% of observations would fall into this category.
```{r}
d <- cooks.distance(full.modelF8F7) # COOK'S DISTANCE
r <- stdres(full.modelF8F7) # STANDARDIZED RESIDUALS
rabs <- abs(r) # absolute stand. resid.
a <- cbind(dataF8F7[,-c(1:19)], d, r, rabs)
asorted <- a[order(-rabs), ]
absresids <- asorted[1:10, ]
kable(absresids, "html") %>% kable_styling("striped") %>% scroll_box(width = "100%")
```
The table shows the 10 highest absolute values of the standardized residuals (rabs). One residual is larger than 3, subject 247. The same subject is identified as an outlier by an outlier test below.

```{r}
# TEST FOR OUTLIERS
outlierTest(full.modelF8F7)
```


#### 1.2 Influential Observations
Cook's Distance. Cut-off points of *4/n*. 
Cook's distance refers to how far, on average, predicted y-values will move if the observation in question is dropped from the data set. It is a measure that combines the information of leverage and the residual of the observation. 18 observations have values of Cook's D above 4/N = 0.019.
```{r}
kable(a[d > 4/N, ], "html") %>% kable_styling("striped") %>% scroll_box(width = "100%")
```

#### 1.3 High leverage observations
Possible high leverage observations are shown in the **Residuals vs Leverage plot**:
```{r}
plot(full.modelF8F7, 5)
```
A data point has high leverage if it has extreme predictor x values. This can be detected by examining the leverage statistic, or the hat-values, which are the diagonal elements of the hat-matrix and which describe the leverage each point has on its fitted values. A value of this statistic above 2(p + 1)/n indicates an observation with high leverage (P. Bruce and Bruce 2017); where, p is the number of predictors and n is the number of observations. Three observations are identified as having high leverage (.hat > 2(p+1)/n)
```{r}
# Create table of model metrics and identify observations with high leverage
model.metrics.F8F7 <- data.frame(augment(full.modelF8F7))
row.names(model.metrics.F8F7) <- model.metrics.F8F7[,1]
model.metrics.F8F7 <- model.metrics.F8F7[, -c(1:19)]
p <- ncol(dataF8F7)
n <- nrow(dataF8F7)
hlt.F8F7<- 2 * (p + 1) / n # high-leverage threshold
model.metrics.F8F7[model.metrics.F8F7$.hat > hlt.F8F7,]
```

### 2. Residuals are Normaly Distributed

The distribution of the residuals can be visualized with a histogram. It appears relatively normal, with one, or a few, outliers on the right tail.
```{r}
resid.modelF8F7 <- residuals(full.modelF8F7)
hist(resid.modelF8F7)
```
The **Q-Q plot** of residuals can be used to inspect the normality assumption. Some observations fall outside of the reference line.
```{r}
plot(full.modelF8F7, 2)
```

Due to the large sample size, a normality test should be able to provide more information. Neither the *Shaprio-Wilks* nor the *Anderson-Darling* test, which is generally considered better than the Shaprio-Wilks, reject the null hypothesis of normally distributed residuals for the model.

```{r}
shapiro.test(resid.modelF8F7)
```
```{r}
ad.test(resid.modelF8F7)
```

### 3. Homoscedasticity
This assumption states that the variance of error terms are similar across the values of the independent variables. A **scale-location** plot of standardized residuals versus predicted values can show whether points are equally distributed across all values of the independent variables.
The plot shows that the variances of the residuals are not entirely constant (heteroscedasticity). The red line appears to be slanting first downwards and then upwards.
```{r}
plot(full.modelF8F7, 3)
```

#### 3.1 Studentized Breusch-Pagan test for heteroscedasticity.
The Breush-Pagan test and can be used to test for heteroscedasticity. It does not indicate heteroscedasticity. However, these tests are not necessarily better than inspecting the plots, so there is a slight cause for concern regarding this assumption.
```{r}
bp <- bptest(full.modelF8F7, studentize = TRUE) # looks good
bp
```

### 4. Linear Relationship Between the Dependent Variable (FAA) and the Independent Variables

#### 4.1 Residuals vs fitted plot
There should be no pattern in the **residual plot**. Ideally, the red line should be approximately flat and the points scattered completely random. This line indicates a slight negative slope for larger fitted values.
Correlations are related to the linearity assumption, see FAA_Correlations.R.
```{r}
plot(full.modelF8F7, 1)
```


### 5. No Multicollinearity
The correlation matrices in FAA_Correlations.r showed low correlations between the independent variables. Variance Inflation Factor (VIF) test shows the same, no collinearity (< 2.5 is very low).
```{r}
# VARIANCE INFLATION FACTOR (VIF)
car::vif(full.modelF8F7)
```

There are a couple of outliers and some influential observations. Several variables (not tested here - see FAA_Correlations.R) are non-normal, which does not matter for regression but for correlations (which is why I use Spearman's rank correlation). The biggest issue for this model is the assumption of normally distributed residuals. The robust variance estimator, often called the Sandwhich estimator, can be used in this case, as it is consistent irrespective of whether the residuals in the regression model have constant variance.

### Run Robust Regression with Huber Weighting Function
```{r}
summary(rr.huber.F8F7 <- rlm(FAA.F8F7 ~ ., data = dataF8F7))
```

### Check Residuals
```{r}
hweights <- data.frame(subject = dataF8F7$ID, resid = rr.huber.F8F7$resid, weight = rr.huber.F8F7$w)
hweights2 <- hweights[order(rr.huber.F8F7$w), ]
hweights2[1:10, ] # We can see that subjects with large residuals get down-weighted
```


### References:
Belsley, D.A., Kuh. E and Welsch, R.E., Regression Diagnostics: Identifying Influential Data and Sources of Collinearity, Wiley, New York, (1980).
Bruce, Peter, and Andrew Bruce. 2017. Practical Statistics for Data Scientists. O’Reilly Media.
James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2014. An Introduction to Statistical Learning: With Applications in R. Springer Publishing Company, Incorporated.

### Useful Links:
[Assumptions of Multiple Linear Regression](https://www.statisticssolutions.com/assumptions-of-multiple-linear-regression/)  
[Regression Diagnostics in R](http://www.sthda.com/english/articles/39-regression-model-diagnostics/161-linear-regression-assumptions-and-diagnostics-in-r-essentials/)  
[Heteroskedasticity and Robust Estimators](http://www3.grips.ac.jp/~yamanota/Lecture_Note_9_Heteroskedasticity)  
[The Robust Sandwich Variance Estimator for Linear Regression in R](http://thestatsgeek.com/2014/02/14/the-robust-sandwich-variance-estimator-for-linear-regression-using-r/)  
[Identifying Unusual Observations in R](https://towardsdatascience.com/how-to-detect-unusual-observations-on-your-regression-model-with-r-de0eaa38bc5b)  
[Robust Linear Regression Review](https://doi.org/10.1080/03610918.2016.1202271)  
[Robust Regression in R](https://stats.idre.ucla.edu/r/dae/robust-regression/)  
[Properites of OLS Estimates](https://www.albert.io/blog/ultimate-properties-of-ols-estimators-guide/)  
[Correlations in R](http://www.sthda.com/english/wiki/correlation-test-between-two-variables-in-r)  
[Graphs in Statistical Analysis](http://ww.w.lithoguru.com/scientist/statistics/Anscombe_Graphs%20in%20Statistical%20Analysis_1973.pdf)  
[Importance of the Normality Assumption](https://doi.org/10.1146/annurev.publhealth.23.100901.140546)  
[P-values vs Confidence Intervals](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2689604/)  